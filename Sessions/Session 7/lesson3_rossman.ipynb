{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lesson3-rossman.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "D7sX8dNBlRYl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Google Colab Setup"
      ]
    },
    {
      "metadata": {
        "id": "-UYMCboElPoo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4081
        },
        "outputId": "5ad5919f-0256-4e95-dd9d-6750ac769363"
      },
      "cell_type": "code",
      "source": [
        "# https://opencv.org/\n",
        "!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python\n",
        "import cv2\n",
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "print(\"accelerator:\", accelerator)\n",
        "!pip install lxml==4.0\n",
        "!pip install http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "\n",
        "# get terminator\n",
        "!pip install fastai\n",
        "\n",
        "# may not be necessary\n",
        "!apt-get install -y libtiff5-dev\n",
        "\n",
        "# on some of the VMs PIL is borked\n",
        "# I am doing this out of abundance of caution\n",
        "#!CC=\"cc -mavx2\" pip install -U --force-reinstall pillow-simd\n",
        "!pip install --force-reinstall scipy\n",
        "!pip install torchvision\n",
        "!pip install Pillow==4.1.1\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 592.3MB 57.3MB/s \n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (3.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/24/f53ff6b61b3d728b90934bddb4f03f8ab584a7f49299bf3bde56e2952612/Pillow-5.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 5.6MB/s \n",
            "\u001b[?25hInstalling collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.2.0 torch-0.3.0.post4 torchvision-0.2.1\n",
            "Collecting fastai\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/6d/9d0d6e17a78b0598d5e8c49a0d03ffc7ff265ae62eca3e2345fab14edb9b/fastai-0.7.0-py3-none-any.whl (112kB)\n",
            "\u001b[K    100% |████████████████████████████████| 122kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from fastai) (16.0.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai) (0.2.1)\n",
            "Collecting plotnine (from fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/49/7af942bc63277dfca6ad397415f4cff60789c56d173b1f7edf0bd30e27e0/plotnine-0.4.0-py2.py3-none-any.whl (3.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.6MB 8.5MB/s \n",
            "\u001b[?25hCollecting jedi (from fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/68/8bbf0ef969095a13ba0d4c77c1945bd86e9811960d052510551d29a2f23b/jedi-0.12.1-py2.py3-none-any.whl (174kB)\n",
            "\u001b[K    100% |████████████████████████████████| 184kB 30.3MB/s \n",
            "\u001b[?25hCollecting isoweek (from fastai)\n",
            "  Downloading https://files.pythonhosted.org/packages/c2/d4/fe7e2637975c476734fcbf53776e650a29680194eb0dd21dbdc020ca92de/isoweek-1.3.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from fastai) (0.7.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fastai) (1.14.6)\n",
            "Collecting sklearn-pandas (from fastai)\n",
            "  Downloading https://files.pythonhosted.org/packages/7e/9c/c94f46b40b86d2c77c46c4c1b858fc66c117b4390665eca28f2e0812db45/sklearn_pandas-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: torch<0.4 in /usr/local/lib/python3.6/dist-packages (from fastai) (0.3.0.post4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from fastai) (0.2.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai) (2.1.2)\n",
            "Collecting jupyter (from fastai)\n",
            "  Downloading https://files.pythonhosted.org/packages/83/df/0f5dd132200728a86190397e1ea87cd76244e42d39ec5e88efd25b2abd7e/jupyter-1.0.0-py2.py3-none-any.whl\n",
            "Collecting graphviz (from fastai)\n",
            "  Downloading https://files.pythonhosted.org/packages/47/87/313cd4ea4f75472826acb74c57f94fc83e04ba93e4ccf35656f6b7f502e2/graphviz-0.9-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai) (5.2.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from fastai) (2018.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from fastai) (3.4.3.18)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.6/dist-packages (from fastai) (4.3.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from fastai) (2.1.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from fastai) (2.5.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from fastai) (0.4)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from fastai) (0.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from fastai) (0.5.1)\n",
            "Requirement already satisfied: Pygments in /usr/local/lib/python3.6/dist-packages (from fastai) (2.1.3)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.6/dist-packages (from fastai) (0.10.0)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.6/dist-packages (from fastai) (2.10)\n",
            "Collecting feather-format (from fastai)\n",
            "  Downloading https://files.pythonhosted.org/packages/08/55/940b97cc6f19a19f5dab9efef2f68a0ce43a7632f858b272391f0b851a7e/feather-format-0.4.0.tar.gz\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from fastai) (3.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fastai) (4.26.0)\n",
            "Collecting torchtext (from fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/90/474d5944d43001a6e72b9aaed5c3e4f77516fbef2317002da2096fd8b5ea/torchtext-0.2.3.tar.gz (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 22.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe in /usr/local/lib/python3.6/dist-packages (from fastai) (1.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.6/dist-packages (from fastai) (0.6.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from fastai) (4.3.0)\n",
            "Collecting bcolz (from fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/23942de9d5c0fb16f10335fa83e52b431bcb8c0d4a8419c9ac206268c279/bcolz-1.2.1.tar.gz (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 15.9MB/s \n",
            "\u001b[?25hCollecting pandas-summary (from fastai)\n",
            "  Downloading https://files.pythonhosted.org/packages/97/55/ea54109a4e7a8e7342bdf23e9382c858224263d984b0d95610568e564f59/pandas_summary-0.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from fastai) (4.5.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from fastai) (5.5.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from fastai) (2.2.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from fastai) (0.1.7)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from fastai) (4.6.1)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from fastai) (1.0.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from fastai) (0.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai) (0.19.1)\n",
            "Collecting widgetsnbextension (from fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/81/35789a3952afb48238289171728072d26d6e76649ddc8b3588657a2d78c1/widgetsnbextension-3.4.2-py2.py3-none-any.whl (2.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.2MB 983kB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from fastai) (2018.8.24)\n",
            "Collecting ipywidgets (from fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/9a/a008c7b1183fac9e52066d80a379b3c64eab535bd9d86cdc29a0b766fd82/ipywidgets-7.4.2-py2.py3-none-any.whl (111kB)\n",
            "\u001b[K    100% |████████████████████████████████| 112kB 31.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: simplegeneric in /usr/local/lib/python3.6/dist-packages (from fastai) (0.8.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from fastai) (2.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai) (0.22.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->fastai) (1.11.0)\n",
            "Collecting mizani>=0.4.5 (from plotnine->fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/01/8a3b4c6e45749674a1e5241174b4b63cd6435125e124bec275f3e02c96ac/mizani-0.4.6-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K    100% |████████████████████████████████| 71kB 27.3MB/s \n",
            "\u001b[?25hCollecting geopandas>=0.3.0 (from plotnine->fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/11/d77c157c16909bd77557d00798b05a5b6615ed60acb5900fbe6a65d35e93/geopandas-0.4.0-py2.py3-none-any.whl (899kB)\n",
            "\u001b[K    100% |████████████████████████████████| 901kB 15.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from plotnine->fastai) (0.8.0)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from plotnine->fastai) (0.5.0)\n",
            "Collecting descartes>=1.1.0 (from plotnine->fastai)\n",
            "  Downloading https://files.pythonhosted.org/packages/e5/b6/1ed2eb03989ae574584664985367ba70cd9cf8b32ee8cad0e8aaeac819f3/descartes-1.1.0-py3-none-any.whl\n",
            "Collecting parso>=0.3.0 (from jedi->fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/51/9c48a46334be50c13d25a3afe55fa05c445699304c5ad32619de953a2305/parso-0.3.1-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 28.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-pandas->fastai) (0.19.2)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->fastai) (5.4.0)\n",
            "Collecting qtconsole (from jupyter->fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/1f/b340d52dee46fbbe8a097dce76d1197258bb599692159d94c80921fef9eb/qtconsole-4.4.1-py2.py3-none-any.whl (112kB)\n",
            "\u001b[K    100% |████████████████████████████████| 112kB 26.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->fastai) (5.2.2)\n",
            "Collecting jupyter-console (from jupyter->fastai)\n",
            "  Downloading https://files.pythonhosted.org/packages/77/82/6469cd7fccf7958cbe5dce2e623f1e3c5e27f1bb1ad36d90519bc2d5d370/jupyter_console-5.2.0-py2.py3-none-any.whl\n",
            "Collecting pyarrow>=0.4.0 (from feather-format->fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/b8/076ebd968f3c6d4e9b6e5de696f0b19964a3d4b7dc020fd32db4e868ad74/pyarrow-0.10.0-cp36-cp36m-manylinux1_x86_64.whl (11.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 11.6MB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->fastai) (2.18.4)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->fastai) (1.0.15)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->fastai) (4.6.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->fastai) (39.1.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->fastai) (5.2.3)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->fastai) (4.4.0)\n",
            "Collecting palettable (from mizani>=0.4.5->plotnine->fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/8a/84537c0354f0d1f03bf644b71bf8e0a50db9c1294181905721a5f3efbf66/palettable-3.1.1-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 25.7MB/s \n",
            "\u001b[?25hCollecting shapely (from geopandas>=0.3.0->plotnine->fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/b6/b53f19062afd49bb5abd049aeed36f13bf8d57ef8f3fa07a5203531a0252/Shapely-1.6.4.post2-cp36-cp36m-manylinux1_x86_64.whl (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 10.7MB/s \n",
            "\u001b[?25hCollecting pyproj (from geopandas>=0.3.0->plotnine->fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/72/5c1888c4948a0c7b736d10e0f0f69966e7c0874a660222ed0a2c2c6daa9f/pyproj-1.9.5.1.tar.gz (4.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.4MB 938kB/s \n",
            "\u001b[?25hCollecting fiona (from geopandas>=0.3.0->plotnine->fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/bf/029958f4e3811ce7017fb5805d5203e8bde6c1816b902964acb2dec67863/Fiona-1.7.13-cp36-cp36m-manylinux1_x86_64.whl (15.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 15.8MB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->fastai) (4.4.0)\n",
            "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->fastai) (0.8.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->fastai) (1.4.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->fastai) (0.5.0)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->fastai) (0.8.1)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->fastai) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->fastai) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->fastai) (3.0.4)\n",
            "Collecting cligj>=0.4 (from fiona->geopandas>=0.3.0->plotnine->fastai)\n",
            "  Downloading https://files.pythonhosted.org/packages/e4/be/30a58b4b0733850280d01f8bd132591b4668ed5c7046761098d665ac2174/cligj-0.5.0-py3-none-any.whl\n",
            "Collecting munch (from fiona->geopandas>=0.3.0->plotnine->fastai)\n",
            "  Downloading https://files.pythonhosted.org/packages/68/f4/260ec98ea840757a0da09e0ed8135333d59b8dfebe9752a365b04857660a/munch-2.3.2.tar.gz\n",
            "Collecting click-plugins (from fiona->geopandas>=0.3.0->plotnine->fastai)\n",
            "  Downloading https://files.pythonhosted.org/packages/95/dd/fef84cf1678418f241ef542c0288bdf215bdd3e35f1fe03dc5223a2e80ba/click_plugins-1.0.4-py2.py3-none-any.whl\n",
            "Collecting click<8,>=4.0 (from cligj>=0.4->fiona->geopandas>=0.3.0->plotnine->fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 21.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: feather-format, torchtext, bcolz, pyproj, munch\n",
            "  Running setup.py bdist_wheel for feather-format ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/85/7d/12/2dfa5c0195f921ac935f5e8f27deada74972edc0ae9988a9c1\n",
            "  Running setup.py bdist_wheel for torchtext ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/42/a6/f4/b267328bde6bb680094a0c173e8e5627ccc99543abded97204\n",
            "  Running setup.py bdist_wheel for bcolz ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/9f/78/26/fb8c0acb91a100dc8914bf236c4eaa4b207cb876893c40b745\n",
            "  Running setup.py bdist_wheel for pyproj ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/89/77/ec/a537585d1022dafde0317dd19d33c4a30d4ee61e19f25ebd8e\n",
            "  Running setup.py bdist_wheel for munch ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/db/bf/bc/06a3e1bfe0ab27d2e720ceb3cff3159398d92644c0cec2c125\n",
            "Successfully built feather-format torchtext bcolz pyproj munch\n",
            "\u001b[31mplotnine 0.4.0 has requirement scipy>=1.0.0, but you'll have scipy 0.19.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: palettable, mizani, shapely, pyproj, click, cligj, munch, click-plugins, fiona, geopandas, descartes, plotnine, parso, jedi, isoweek, sklearn-pandas, qtconsole, widgetsnbextension, ipywidgets, jupyter-console, jupyter, graphviz, pyarrow, feather-format, torchtext, bcolz, pandas-summary, fastai\n",
            "Successfully installed bcolz-1.2.1 click-7.0 click-plugins-1.0.4 cligj-0.5.0 descartes-1.1.0 fastai-0.7.0 feather-format-0.4.0 fiona-1.7.13 geopandas-0.4.0 graphviz-0.9 ipywidgets-7.4.2 isoweek-1.3.3 jedi-0.12.1 jupyter-1.0.0 jupyter-console-5.2.0 mizani-0.4.6 munch-2.3.2 palettable-3.1.1 pandas-summary-0.0.5 parso-0.3.1 plotnine-0.4.0 pyarrow-0.10.0 pyproj-1.9.5.1 qtconsole-4.4.1 shapely-1.6.4.post2 sklearn-pandas-1.7.0 torchtext-0.2.3 widgetsnbextension-3.4.2\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libjbig-dev libjbig0 liblzma-dev libtiff5 libtiffxx5\n",
            "Suggested packages:\n",
            "  liblzma-doc\n",
            "The following NEW packages will be installed:\n",
            "  libjbig-dev libjbig0 liblzma-dev libtiff5 libtiff5-dev libtiffxx5\n",
            "0 upgraded, 6 newly installed, 0 to remove and 0 not upgraded.\n",
            "Need to get 624 kB of archives.\n",
            "After this operation, 2,418 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu artful/main amd64 libjbig0 amd64 2.1-3.1 [26.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu artful-updates/main amd64 libtiff5 amd64 4.0.8-5ubuntu0.1 [150 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu artful/main amd64 libjbig-dev amd64 2.1-3.1 [24.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu artful/main amd64 liblzma-dev amd64 5.2.2-1.3 [145 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu artful-updates/main amd64 libtiffxx5 amd64 4.0.8-5ubuntu0.1 [5,724 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu artful-updates/main amd64 libtiff5-dev amd64 4.0.8-5ubuntu0.1 [271 kB]\n",
            "Fetched 624 kB in 0s (5,953 kB/s)\n",
            "Selecting previously unselected package libjbig0:amd64.\n",
            "(Reading database ... 18408 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libjbig0_2.1-3.1_amd64.deb ...\n",
            "Unpacking libjbig0:amd64 (2.1-3.1) ...\n",
            "Selecting previously unselected package libtiff5:amd64.\n",
            "Preparing to unpack .../1-libtiff5_4.0.8-5ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtiff5:amd64 (4.0.8-5ubuntu0.1) ...\n",
            "Selecting previously unselected package libjbig-dev:amd64.\n",
            "Preparing to unpack .../2-libjbig-dev_2.1-3.1_amd64.deb ...\n",
            "Unpacking libjbig-dev:amd64 (2.1-3.1) ...\n",
            "Selecting previously unselected package liblzma-dev:amd64.\n",
            "Preparing to unpack .../3-liblzma-dev_5.2.2-1.3_amd64.deb ...\n",
            "Unpacking liblzma-dev:amd64 (5.2.2-1.3) ...\n",
            "Selecting previously unselected package libtiffxx5:amd64.\n",
            "Preparing to unpack .../4-libtiffxx5_4.0.8-5ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtiffxx5:amd64 (4.0.8-5ubuntu0.1) ...\n",
            "Selecting previously unselected package libtiff5-dev:amd64.\n",
            "Preparing to unpack .../5-libtiff5-dev_4.0.8-5ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtiff5-dev:amd64 (4.0.8-5ubuntu0.1) ...\n",
            "Setting up libjbig0:amd64 (2.1-3.1) ...\n",
            "Setting up libtiff5:amd64 (4.0.8-5ubuntu0.1) ...\n",
            "Setting up libjbig-dev:amd64 (2.1-3.1) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "Setting up libtiffxx5:amd64 (4.0.8-5ubuntu0.1) ...\n",
            "Setting up liblzma-dev:amd64 (5.2.2-1.3) ...\n",
            "Setting up libtiff5-dev:amd64 (4.0.8-5ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "Collecting scipy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 31.2MB 1.3MB/s \n",
            "\u001b[?25hCollecting numpy>=1.8.2 (from scipy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/02/bae88c4aaea4256d890adbf3f7cf33e59a443f9985cf91cd08a35656676a/numpy-1.15.2-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 13.9MB 4.2MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy, scipy\n",
            "  Found existing installation: numpy 1.14.6\n",
            "    Uninstalling numpy-1.14.6:\n",
            "      Successfully uninstalled numpy-1.14.6\n",
            "  Found existing installation: scipy 0.19.1\n",
            "    Uninstalling scipy-0.19.1:\n",
            "      Successfully uninstalled scipy-0.19.1\n",
            "Successfully installed numpy-1.15.2 scipy-1.1.0\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.3.0.post4)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.15.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch->torchvision) (3.13)\n",
            "Collecting Pillow==4.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/e5/88b3d60924a3f8476fa74ec086f5fbaba56dd6cee0d82845f883b6b6dd18/Pillow-4.1.1-cp36-cp36m-manylinux1_x86_64.whl (5.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.7MB 925kB/s \n",
            "\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.1.1) (0.46)\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 5.2.0\n",
            "    Uninstalling Pillow-5.2.0:\n",
            "      Successfully uninstalled Pillow-5.2.0\n",
            "Successfully installed Pillow-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w89ZK6bI8fTH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://files.fast.ai/part2/lesson14/rossmann.tgz\n",
        "!mkdir -p data/rossmann/ ; mv rossmann.tgz data/rossmann/ ; cd data/rossmann/ ; tar -xvzf rossmann.tgz\n",
        "!ls data/rossmann/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HK1efowvlNio",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Structured and time series data"
      ]
    },
    {
      "metadata": {
        "id": "oXOowV5olNiq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook contains an implementation of the third place result in the Rossman Kaggle competition as detailed in Guo/Berkhahn's [Entity Embeddings of Categorical Variables](https://arxiv.org/abs/1604.06737).\n",
        "\n",
        "The motivation behind exploring this architecture is it's relevance to real-world application. Most data used for decision making day-to-day in industry is structured and/or time-series data. Here we explore the end-to-end process of using neural networks with practical structured data problems."
      ]
    },
    {
      "metadata": {
        "id": "N4t4NIUYlNiq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "54DGZsYslNiu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from fastai.structured import *\n",
        "from fastai.column_data import *\n",
        "np.set_printoptions(threshold=50, edgeitems=20)\n",
        "\n",
        "PATH='data/rossmann/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rjx6wecalNiw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create datasets"
      ]
    },
    {
      "metadata": {
        "id": "5q5ODNJPlNix",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In addition to the provided data, we will be using external datasets put together by participants in the Kaggle competition. You can download all of them [here](http://files.fast.ai/part2/lesson14/rossmann.tgz).\n",
        "\n",
        "For completeness, the implementation used to put them together is included below."
      ]
    },
    {
      "metadata": {
        "id": "jq6HrnMHlNiy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def concat_csvs(dirname):\n",
        "    path = f'{PATH}{dirname}'\n",
        "    filenames=glob(f\"{PATH}/*.csv\")\n",
        "\n",
        "    wrote_header = False\n",
        "    with open(f\"{path}.csv\",\"w\") as outputfile:\n",
        "        for filename in filenames:\n",
        "            name = filename.split(\".\")[0]\n",
        "            with open(filename) as f:\n",
        "                line = f.readline()\n",
        "                if not wrote_header:\n",
        "                    wrote_header = True\n",
        "                    outputfile.write(\"file,\"+line)\n",
        "                for line in f:\n",
        "                     outputfile.write(name + \",\" + line)\n",
        "                outputfile.write(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PtbOWI8olNi0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# concat_csvs('googletrend')\n",
        "# concat_csvs('weather')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4WXLn76PlNi8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Feature Space:\n",
        "* train: Training set provided by competition\n",
        "* store: List of stores\n",
        "* store_states: mapping of store to the German state they are in\n",
        "* List of German state names\n",
        "* googletrend: trend of certain google keywords over time, found by users to correlate well w/ given data\n",
        "* weather: weather\n",
        "* test: testing set"
      ]
    },
    {
      "metadata": {
        "id": "tnDnQgBzlNi9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "table_names = ['train', 'store', 'store_states', 'state_names', \n",
        "               'googletrend', 'weather', 'test']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZyEk02jelNi_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll be using the popular data manipulation framework `pandas`. Among other things, pandas allows you to manipulate tables/data frames in python as one would in a database.\n",
        "\n",
        "We're going to go ahead and load all of our csv's as dataframes into the list `tables`."
      ]
    },
    {
      "metadata": {
        "id": "Z3I3WV4_lNi_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tables = [pd.read_csv(f'{PATH}{fname}.csv', low_memory=False) for fname in table_names]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XUzR_9D3lNjB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dZJNVGnHlNjF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can use `head()` to get a quick look at the contents of each table:\n",
        "* train: Contains store information on a daily basis, tracks things like sales, customers, whether that day was a holdiay, etc.\n",
        "* store: general info about the store including competition, etc.\n",
        "* store_states: maps store to state it is in\n",
        "* state_names: Maps state abbreviations to names\n",
        "* googletrend: trend data for particular week/state\n",
        "* weather: weather conditions for each state\n",
        "* test: Same as training table, w/o sales and customers\n"
      ]
    },
    {
      "metadata": {
        "id": "qWer4ACNlNjG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for t in tables: display(t.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pDpMa9BYlNjM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is very representative of a typical industry dataset.\n",
        "\n",
        "The following returns summarized aggregate information to each table accross each field."
      ]
    },
    {
      "metadata": {
        "id": "VclFmxxYlNjM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for t in tables: display(DataFrameSummary(t).summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aCrf21aQlNjP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning / Feature Engineering"
      ]
    },
    {
      "metadata": {
        "id": "fn4e5nmllNjP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As a structured data problem, we necessarily have to go through all the cleaning and feature engineering, even though we're using a neural network."
      ]
    },
    {
      "metadata": {
        "id": "fcjzsxETlNjQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train, store, store_states, state_names, googletrend, weather, test = tables"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xNc0udjhlNjS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(train),len(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TE_1eoM3lNjV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We turn state Holidays to booleans, to make them more convenient for modeling. We can do calculations on pandas fields using notation very similar (often identical) to numpy."
      ]
    },
    {
      "metadata": {
        "id": "eekxm1cVlNjV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train.StateHoliday = train.StateHoliday!='0'\n",
        "test.StateHoliday = test.StateHoliday!='0'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8O_G7rylNjY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`join_df` is a function for joining tables on specific fields. By default, we'll be doing a left outer join of `right` on the `left` argument using the given fields for each table.\n",
        "\n",
        "Pandas does joins using the `merge` method. The `suffixes` argument describes the naming convention for duplicate fields. We've elected to leave the duplicate field names on the left untouched, and append a \"\\_y\" to those on the right."
      ]
    },
    {
      "metadata": {
        "id": "SHuiMYlOlNjZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def join_df(left, right, left_on, right_on=None, suffix='_y'):\n",
        "    if right_on is None: right_on = left_on\n",
        "    return left.merge(right, how='left', left_on=left_on, right_on=right_on, \n",
        "                      suffixes=(\"\", suffix))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ooSK2qrElNjb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Join weather/state names."
      ]
    },
    {
      "metadata": {
        "id": "sCMPiau_lNjb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weather = join_df(weather, state_names, \"file\", \"StateName\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u8BRTSnklNjm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In pandas you can add new columns to a dataframe by simply defining it. We'll do this for googletrends by extracting dates and state names from the given data and adding those columns.\n",
        "\n",
        "We're also going to replace all instances of state name 'NI' to match the usage in the rest of the data: 'HB,NI'. This is a good opportunity to highlight pandas indexing. We can use `.loc[rows, cols]` to select a list of rows and a list of columns from the dataframe. In this case, we're selecting rows w/ statename 'NI' by using a boolean list `googletrend.State=='NI'` and selecting \"State\"."
      ]
    },
    {
      "metadata": {
        "id": "u0G_1UR9lNjn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "googletrend['Date'] = googletrend.week.str.split(' - ', expand=True)[0]\n",
        "googletrend['State'] = googletrend.file.str.split('_', expand=True)[2]\n",
        "googletrend.loc[googletrend.State=='NI', \"State\"] = 'HB,NI'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qoBMYzuPlNjs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following extracts particular date fields from a complete datetime for the purpose of constructing categoricals.\n",
        "\n",
        "You should *always* consider this feature extraction step when working with date-time. Without expanding your date-time into these additional fields, you can't capture any trend/cyclical behavior as a function of time at any of these granularities. We'll add to every table with a date field."
      ]
    },
    {
      "metadata": {
        "id": "IA4PDvlplNj6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "add_datepart(weather, \"Date\", drop=False)\n",
        "add_datepart(googletrend, \"Date\", drop=False)\n",
        "add_datepart(train, \"Date\", drop=False)\n",
        "add_datepart(test, \"Date\", drop=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7JK0NXwolNj8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Google trends data has a special category for the whole of the Germany - we'll pull that out so we can use it explicitly."
      ]
    },
    {
      "metadata": {
        "id": "fKz5QuLJlNj9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trend_de = googletrend[googletrend.file == 'Rossmann_DE']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "948XficblNj_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can outer join all of our data into a single dataframe. Recall that in outer joins everytime a value in the joining field on the left table does not have a corresponding value on the right table, the corresponding row in the new table has Null values for all right table fields. One way to check that all records are consistent and complete is to check for Null values post-join, as we do here.\n",
        "\n",
        "*Aside*: Why note just do an inner join?\n",
        "If you are assuming that all records are complete and match on the field you desire, an inner join will do the same thing as an outer join. However, in the event you are wrong or a mistake is made, an outer join followed by a null-check will catch it. (Comparing before/after # of rows for inner join is equivalent, but requires keeping track of before/after row #'s. Outer join is easier.)"
      ]
    },
    {
      "metadata": {
        "id": "62ejy9jAlNj_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "store = join_df(store, store_states, \"Store\")\n",
        "len(store[store.State.isnull()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZJHIIZVJlNkC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "joined = join_df(train, store, \"Store\")\n",
        "joined_test = join_df(test, store, \"Store\")\n",
        "len(joined[joined.StoreType.isnull()]),len(joined_test[joined_test.StoreType.isnull()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cAvAIiTflNkE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "joined = join_df(joined, googletrend, [\"State\",\"Year\", \"Week\"])\n",
        "joined_test = join_df(joined_test, googletrend, [\"State\",\"Year\", \"Week\"])\n",
        "len(joined[joined.trend.isnull()]),len(joined_test[joined_test.trend.isnull()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sch-k3tilNkG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "joined = joined.merge(trend_de, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\n",
        "joined_test = joined_test.merge(trend_de, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\n",
        "len(joined[joined.trend_DE.isnull()]),len(joined_test[joined_test.trend_DE.isnull()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xhUt9KB4lNkH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "joined = join_df(joined, weather, [\"State\",\"Date\"])\n",
        "joined_test = join_df(joined_test, weather, [\"State\",\"Date\"])\n",
        "len(joined[joined.Mean_TemperatureC.isnull()]),len(joined_test[joined_test.Mean_TemperatureC.isnull()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ve8BYHuXlNkK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for df in (joined, joined_test):\n",
        "    for c in df.columns:\n",
        "        if c.endswith('_y'):\n",
        "            if c in df.columns: df.drop(c, inplace=True, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "akF9tLfVlNkM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we'll fill in missing values to avoid complications with `NA`'s. `NA` (not available) is how Pandas indicates missing values; many models have problems when missing values are present, so it's always important to think about how to deal with them. In these cases, we are picking an arbitrary *signal value* that doesn't otherwise appear in the data."
      ]
    },
    {
      "metadata": {
        "id": "U-yw6Q9QlNkM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for df in (joined,joined_test):\n",
        "    df['CompetitionOpenSinceYear'] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32)\n",
        "    df['CompetitionOpenSinceMonth'] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32)\n",
        "    df['Promo2SinceYear'] = df.Promo2SinceYear.fillna(1900).astype(np.int32)\n",
        "    df['Promo2SinceWeek'] = df.Promo2SinceWeek.fillna(1).astype(np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "veJ8UJT1lNkO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we'll extract features \"CompetitionOpenSince\" and \"CompetitionDaysOpen\". Note the use of `apply()` in mapping a function across dataframe values."
      ]
    },
    {
      "metadata": {
        "id": "v5KmdW3alNkO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for df in (joined,joined_test):\n",
        "    df[\"CompetitionOpenSince\"] = pd.to_datetime(dict(year=df.CompetitionOpenSinceYear, \n",
        "                                                     month=df.CompetitionOpenSinceMonth, day=15))\n",
        "    df[\"CompetitionDaysOpen\"] = df.Date.subtract(df.CompetitionOpenSince).dt.days"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YJrQK3ySlNkQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll replace some erroneous / outlying data."
      ]
    },
    {
      "metadata": {
        "id": "X4n_yB0elNkQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for df in (joined,joined_test):\n",
        "    df.loc[df.CompetitionDaysOpen<0, \"CompetitionDaysOpen\"] = 0\n",
        "    df.loc[df.CompetitionOpenSinceYear<1990, \"CompetitionDaysOpen\"] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4apphRwMlNkT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We add \"CompetitionMonthsOpen\" field, limiting the maximum to 2 years to limit number of unique categories."
      ]
    },
    {
      "metadata": {
        "id": "4YREw3cLlNkU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for df in (joined,joined_test):\n",
        "    df[\"CompetitionMonthsOpen\"] = df[\"CompetitionDaysOpen\"]//30\n",
        "    df.loc[df.CompetitionMonthsOpen>24, \"CompetitionMonthsOpen\"] = 24\n",
        "joined.CompetitionMonthsOpen.unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pmPRHgxblNkX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Same process for Promo dates."
      ]
    },
    {
      "metadata": {
        "id": "03XQzY-UlNkX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for df in (joined,joined_test):\n",
        "    df[\"Promo2Since\"] = pd.to_datetime(df.apply(lambda x: Week(\n",
        "        x.Promo2SinceYear, x.Promo2SinceWeek).monday(), axis=1).astype(pd.datetime))\n",
        "    df[\"Promo2Days\"] = df.Date.subtract(df[\"Promo2Since\"]).dt.days"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5AFlnyROlNkZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for df in (joined,joined_test):\n",
        "    df.loc[df.Promo2Days<0, \"Promo2Days\"] = 0\n",
        "    df.loc[df.Promo2SinceYear<1990, \"Promo2Days\"] = 0\n",
        "    df[\"Promo2Weeks\"] = df[\"Promo2Days\"]//7\n",
        "    df.loc[df.Promo2Weeks<0, \"Promo2Weeks\"] = 0\n",
        "    df.loc[df.Promo2Weeks>25, \"Promo2Weeks\"] = 25\n",
        "    df.Promo2Weeks.unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u2WFEMJSrIs-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.version.version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "blgNOyiHlNka",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "joined.to_feather(f'{PATH}joined')\n",
        "joined_test.to_feather(f'{PATH}joined_test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KP4jZ1lalNkc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Durations"
      ]
    },
    {
      "metadata": {
        "id": "xNoBA4t-lNkc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is common when working with time series data to extract data that explains relationships across rows as opposed to columns, e.g.:\n",
        "* Running averages\n",
        "* Time until next event\n",
        "* Time since last event\n",
        "\n",
        "This is often difficult to do with most table manipulation frameworks, since they are designed to work with relationships across columns. As such, we've created a class to handle this type of data.\n",
        "\n",
        "We'll define a function `get_elapsed` for cumulative counting across a sorted dataframe. Given a particular field `fld` to monitor, this function will start tracking time since the last occurrence of that field. When the field is seen again, the counter is set to zero.\n",
        "\n",
        "Upon initialization, this will result in datetime na's until the field is encountered. This is reset every time a new store is seen. We'll see how to use this shortly."
      ]
    },
    {
      "metadata": {
        "id": "IcihSz1IlNkd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_elapsed(fld, pre):\n",
        "    day1 = np.timedelta64(1, 'D')\n",
        "    last_date = np.datetime64()\n",
        "    last_store = 0\n",
        "    res = []\n",
        "\n",
        "    for s,v,d in zip(df.Store.values,df[fld].values, df.Date.values):\n",
        "        if s != last_store:\n",
        "            last_date = np.datetime64()\n",
        "            last_store = s\n",
        "        if v: last_date = d\n",
        "        res.append(((d-last_date).astype('timedelta64[D]') / day1))\n",
        "    df[pre+fld] = res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iqcFFWPklNkf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll be applying this to a subset of columns:"
      ]
    },
    {
      "metadata": {
        "id": "yiXbKTcRlNkg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "columns = [\"Date\", \"Store\", \"Promo\", \"StateHoliday\", \"SchoolHoliday\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iPErTkJnlNkl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = train[columns]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a4gr7YV4lNkn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = test[columns]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8NWAymw0lNkp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's walk through an example.\n",
        "\n",
        "Say we're looking at School Holiday. We'll first sort by Store, then Date, and then call `add_elapsed('SchoolHoliday', 'After')`:\n",
        "This will apply to each row with School Holiday:\n",
        "* A applied to every row of the dataframe in order of store and date\n",
        "* Will add to the dataframe the days since seeing a School Holiday\n",
        "* If we sort in the other direction, this will count the days until another holiday."
      ]
    },
    {
      "metadata": {
        "id": "2yAR5cXPlNkp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fld = 'SchoolHoliday'\n",
        "df = df.sort_values(['Store', 'Date'])\n",
        "get_elapsed(fld, 'After')\n",
        "df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n",
        "get_elapsed(fld, 'Before')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N_XedQY7lNkr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll do this for two more fields."
      ]
    },
    {
      "metadata": {
        "id": "cbGSZenglNkr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fld = 'StateHoliday'\n",
        "df = df.sort_values(['Store', 'Date'])\n",
        "get_elapsed(fld, 'After')\n",
        "df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n",
        "get_elapsed(fld, 'Before')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gye0AWuklNkt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fld = 'Promo'\n",
        "df = df.sort_values(['Store', 'Date'])\n",
        "get_elapsed(fld, 'After')\n",
        "df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n",
        "get_elapsed(fld, 'Before')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l3RePVg7lNku",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We're going to set the active index to Date."
      ]
    },
    {
      "metadata": {
        "id": "Xk1VwkcslNkv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = df.set_index(\"Date\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Pa8IIK3lNkw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then set null values from elapsed field calculations to 0."
      ]
    },
    {
      "metadata": {
        "id": "4PxBiRV5lNky",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "columns = ['SchoolHoliday', 'StateHoliday', 'Promo']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZPGAMNKvlNk0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for o in ['Before', 'After']:\n",
        "    for p in columns:\n",
        "        a = o+p\n",
        "        df[a] = df[a].fillna(0).astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gTcq0-7IlNk1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we'll demonstrate window functions in pandas to calculate rolling quantities.\n",
        "\n",
        "Here we're sorting by date (`sort_index()`) and counting the number of events of interest (`sum()`) defined in `columns` in the following week (`rolling()`), grouped by Store (`groupby()`). We do the same in the opposite direction."
      ]
    },
    {
      "metadata": {
        "id": "lqvnznSnlNk2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bwd = df[['Store']+columns].sort_index().groupby(\"Store\").rolling(7, min_periods=1).sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xVB5c1ojlNk3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fwd = df[['Store']+columns].sort_index(ascending=False\n",
        "                                      ).groupby(\"Store\").rolling(7, min_periods=1).sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N4Atspn1lNk5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we want to drop the Store indices grouped together in the window function.\n",
        "\n",
        "Often in pandas, there is an option to do this in place. This is time and memory efficient when working with large datasets."
      ]
    },
    {
      "metadata": {
        "id": "Kp1FHTonlNk6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bwd.drop('Store',1,inplace=True)\n",
        "bwd.reset_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CSNfDgNYlNk8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fwd.drop('Store',1,inplace=True)\n",
        "fwd.reset_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zc7sVl9MlNk-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.reset_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nyHXUoESlNlB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we'll merge these values onto the df."
      ]
    },
    {
      "metadata": {
        "id": "w61NXMh9lNlC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = df.merge(bwd, 'left', ['Date', 'Store'], suffixes=['', '_bw'])\n",
        "df = df.merge(fwd, 'left', ['Date', 'Store'], suffixes=['', '_fw'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qiBOp4rTlNlF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.drop(columns,1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5c4mdcgIlNlI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wr2Tso0ClNlN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It's usually a good idea to back up large tables of extracted / wrangled features before you join them onto another one, that way you can go back to it easily if you need to make changes to it."
      ]
    },
    {
      "metadata": {
        "id": "JCfwlJRvlNlN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.to_feather(f'{PATH}df')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yS7ENc80lNlP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_feather(f'{PATH}df')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o33uXydNlNlR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df[\"Date\"] = pd.to_datetime(df.Date)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oT8f4oFylNlT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eMxqOx8PlNlW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "joined = join_df(joined, df, ['Store', 'Date'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Eg7IRx_lNlX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "joined_test = join_df(joined_test, df, ['Store', 'Date'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RVIO4dxflNlY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The authors also removed all instances where the store had zero sale / was closed. We speculate that this may have cost them a higher standing in the competition. One reason this may be the case is that a little exploratory data analysis reveals that there are often periods where stores are closed, typically for refurbishment. Before and after these periods, there are naturally spikes in sales that one might expect. By ommitting this data from their training, the authors gave up the ability to leverage information about these periods to predict this otherwise volatile behavior."
      ]
    },
    {
      "metadata": {
        "id": "S4v3pzo8lNlZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "joined = joined[joined.Sales!=0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vAsSatSOlNla",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll back this up as well."
      ]
    },
    {
      "metadata": {
        "id": "gFwJFA8WlNla",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "joined.reset_index(inplace=True)\n",
        "joined_test.reset_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xUnZxcablNlb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "joined.to_feather(f'{PATH}joined')\n",
        "joined_test.to_feather(f'{PATH}joined_test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-16uZ6ZulNld",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now have our final set of engineered features.\n",
        "\n",
        "While these steps were explicitly outlined in the paper, these are all fairly typical feature engineering steps for dealing with time series data and are practical in any similar setting."
      ]
    },
    {
      "metadata": {
        "id": "uFngK702lNle",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create features"
      ]
    },
    {
      "metadata": {
        "id": "SShfqCvBlNlf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "joined = pd.read_feather(f'{PATH}joined')\n",
        "joined_test = pd.read_feather(f'{PATH}joined_test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AHbrCfJdlNlf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "joined.head().T.head(40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6PS27LmilNlh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we've engineered all our features, we need to convert to input compatible with a neural network.\n",
        "\n",
        "This includes converting categorical variables into contiguous integers or one-hot encodings, normalizing continuous features to standard normal, etc..."
      ]
    },
    {
      "metadata": {
        "id": "18-u_EPPlNlh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
        "    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
        "    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
        "    'SchoolHoliday_fw', 'SchoolHoliday_bw']\n",
        "\n",
        "contin_vars = ['CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
        "   'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
        "   'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
        "   'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday']\n",
        "\n",
        "n = len(joined); "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_OYth7klNlj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dep = 'Sales'\n",
        "joined = joined[cat_vars+contin_vars+[dep, 'Date']].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XkrgZAqElNlk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "joined_test[dep] = 0\n",
        "joined_test = joined_test[cat_vars+contin_vars+[dep, 'Date', 'Id']].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zcnu5UKOlNlm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for v in cat_vars: joined[v] = joined[v].astype('category').cat.as_ordered()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xb1q0ZkllNlo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "apply_cats(joined_test, joined)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gkWqnrGGlNlq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for v in contin_vars:\n",
        "    joined[v] = joined[v].fillna(0).astype('float32')\n",
        "    joined_test[v] = joined_test[v].fillna(0).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mtMzATQFlNlq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We're going to run on a sample."
      ]
    },
    {
      "metadata": {
        "id": "wQ1LlFvjlNlq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "idxs = get_cv_idxs(n, val_pct=150000/n)\n",
        "joined_samp = joined.iloc[idxs].set_index(\"Date\")\n",
        "samp_size = len(joined_samp); samp_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j_mgb3jzlNls",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To run on the full dataset, use this instead:"
      ]
    },
    {
      "metadata": {
        "id": "Y7L4bmhPlNls",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "samp_size = n\n",
        "joined_samp = joined.set_index(\"Date\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VWHoJCevlNlu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now process our data..."
      ]
    },
    {
      "metadata": {
        "id": "yrluMTY2lNlu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "joined_samp.head(2)\n",
        "#joined_samp.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4n4ZFJKTlNlw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df, y, nas, mapper = proc_df(joined_samp, 'Sales', do_scale=True)\n",
        "yl = np.log(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WvMWqQMjlNlx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "joined_test = joined_test.set_index(\"Date\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GmM7kvBX0HJo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "joined_test.head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GeNqHoRDlNl0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test, _, nas, mapper = proc_df(joined_test, 'Sales', do_scale=True, skip_flds=['Id'],\n",
        "                                  mapper=mapper, na_dict=nas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lNCVjMIElNl1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LaKAP30YlNl3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In time series data, cross-validation is not random. Instead, our holdout data is generally the most recent data, as it would be in real application. This issue is discussed in detail in [this post](http://www.fast.ai/2017/11/13/validation-sets/) on our web site.\n",
        "\n",
        "One approach is to take the last 25% of rows (sorted by date) as our validation set."
      ]
    },
    {
      "metadata": {
        "id": "O3R_kDfylNl3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_ratio = 0.75\n",
        "# train_ratio = 0.9\n",
        "train_size = int(samp_size * train_ratio); train_size\n",
        "val_idx = list(range(train_size, len(df)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TnDbfqkflNl3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "An even better option for picking a validation set is using the exact same length of time period as the test set uses - this is implemented here:"
      ]
    },
    {
      "metadata": {
        "id": "VBNPklvrlNl4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "val_idx = np.flatnonzero(\n",
        "    (df.index<=datetime.datetime(2014,9,17)) & (df.index>=datetime.datetime(2014,8,1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lVuZpclDlNl5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "val_idx=[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-4QlAiTQlNl7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## DL"
      ]
    },
    {
      "metadata": {
        "id": "7j-3toNelNl7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We're ready to put together our models.\n",
        "\n",
        "Root-mean-squared percent error is the metric Kaggle used for this competition.\n",
        "\n",
        "https://www.kaggle.com/c/rossmann-store-sales#evaluation"
      ]
    },
    {
      "metadata": {
        "id": "xzc7idWalNl8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def inv_y(a): return np.exp(a)\n",
        "\n",
        "def exp_rmspe(y_pred, targ):\n",
        "    targ = inv_y(targ)\n",
        "    pct_var = (targ - inv_y(y_pred))/targ\n",
        "    return math.sqrt((pct_var**2).mean())\n",
        "\n",
        "max_log_y = np.max(yl)\n",
        "y_range = (0, max_log_y*1.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ycGP_4xhlNl9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can create a ModelData object directly from out data frame."
      ]
    },
    {
      "metadata": {
        "id": "hCgVDgBblNl-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "md = ColumnarModelData.from_data_frame(PATH, val_idx, df, yl.astype(np.float32), cat_flds=cat_vars, bs=128,\n",
        "                                       test_df=df_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QqrzfY_blNl-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Some categorical variables have a lot more levels than others. Store, in particular, has over a thousand!"
      ]
    },
    {
      "metadata": {
        "id": "LXeIdVuxlNl-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cat_sz = [(c, len(joined_samp[c].cat.categories)+1) for c in cat_vars]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7S_JBjO2lNl_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cat_sz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tHDFzptqlNmA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We use the *cardinality* of each variable (that is, its number of unique values) to decide how large to make its *embeddings*. Each level will be associated with a vector with length defined as below."
      ]
    },
    {
      "metadata": {
        "id": "nvJZ_rtvlNmA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "emb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RuTUmQg5lNmB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "emb_szs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dp3cnWzolNmC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars),\n",
        "                   0.04, 1, [1000,500], [0.001,0.01], y_range=y_range)\n",
        "lr = 1e-3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dzr9vfLVlNmE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m.lr_find()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jkLKxMHPlNmF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m.sched.plot(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UL_tbXl3lNmG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Sample"
      ]
    },
    {
      "metadata": {
        "id": "0IKJk5OFlNmG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars),\n",
        "                   0.04, 1, [1000,500], [0.001,0.01], y_range=y_range)\n",
        "lr = 1e-3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YUo5G2kblNmI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m.fit(lr, 3, metrics=[exp_rmspe])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yfPBG6hGlNmJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m.fit(lr, 5, metrics=[exp_rmspe], cycle_len=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pUa0WXMslNmL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m.fit(lr, 2, metrics=[exp_rmspe], cycle_len=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XjdGkyPplNmM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### All"
      ]
    },
    {
      "metadata": {
        "id": "PlNNi0jGlNmM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars),\n",
        "                   0.04, 1, [1000,500], [0.001,0.01], y_range=y_range)\n",
        "lr = 1e-3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j1QPRY1PlNmN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m.fit(lr, 1, metrics=[exp_rmspe])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H7EXdCJIlNmO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m.fit(lr, 3, metrics=[exp_rmspe])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hohIxZ0UlNmP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m.fit(lr, 3, metrics=[exp_rmspe], cycle_len=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ngpSr5vnlNmQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test"
      ]
    },
    {
      "metadata": {
        "id": "ba8Nl2Q7lNmQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars),\n",
        "                   0.04, 1, [1000,500], [0.001,0.01], y_range=y_range)\n",
        "lr = 1e-3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ARh2bD7flNmR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m.fit(lr, 3, metrics=[exp_rmspe])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SlY8-b2HlNmS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m.fit(lr, 3, metrics=[exp_rmspe], cycle_len=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZkgwtGuvlNmT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m.save('val0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7xSw5tPWlNmW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m.load('val0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rPKxIFA_lNmW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x,y=m.predict_with_targs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dFadhgh3lNmX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "exp_rmspe(x,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "db4BT1i6lNma",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred_test=m.predict(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "51q1azzMlNma",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred_test = np.exp(pred_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LF_D-yWvlNmb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "joined_test['Sales']=pred_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eKXW5SLNlNmc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "csv_fn=f'{PATH}tmp/sub.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BnnLFenalNmd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "joined_test[['Id','Sales']].to_csv(csv_fn, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z8R2IHo_lNme",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "FileLink(csv_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qSVsRnt5lNmg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## RF"
      ]
    },
    {
      "metadata": {
        "id": "RYKjgnwKlNmg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ta57BGGzlNmh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "((val,trn), (y_val,y_trn)) = split_by_idx(val_idx, df.values, yl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EYTpIaiKlNmj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m = RandomForestRegressor(n_estimators=40, max_features=0.99, min_samples_leaf=2,\n",
        "                          n_jobs=-1, oob_score=True)\n",
        "m.fit(trn, y_trn);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qxw6tqOVlNmk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preds = m.predict(val)\n",
        "m.score(trn, y_trn), m.score(val, y_val), m.oob_score_, exp_rmspe(preds, y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SZX1l-cRlNmk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}